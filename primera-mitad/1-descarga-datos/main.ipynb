{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238986ab-5e14-44ee-a70e-b5043ffc58b2",
   "metadata": {},
   "source": [
    "# Descargando los datos\n",
    "Para todos los problemas de nuestro mundo es mejor tener datos y basar nuestras decisiones en ellos. Conseguir los datos es lo dificil!\n",
    "\n",
    "Hemos dicho que queremos aprender sobre la popularidad de tweets - entonces tiene sentido tirarnos directamente hacia este lado y investigar sobre datos de tweets populares!\n",
    "\n",
    "## Tarea: Como podemos aportar valor al mundo a traves de los datos?\n",
    "\n",
    "1. Como podemos usar datos para conseguir valor real en el mundo?\n",
    "2. Que otras piezas del puzle hay en la \"cadena de valor\"?\n",
    "3. Que implica esto sobre nuestra metodologia?\n",
    "\n",
    "* Apoyo a decisiones - mejores decisiones, entender el contexto, experimentar, tener mas seguridad, automatizar (decisiones y flujos)...\n",
    "* Los otros stakeholders y como impactamos sobre nuestro \"cliente final\"\n",
    "* Iteracion rapida...\n",
    "\n",
    "## Tarea: investigar Twitter:\n",
    "\n",
    "1. Que cosas podrian ser utiles considerar? Que tipo de cosas podemos querer en nuestra base de datos?\n",
    "2. Como podemos recoger estos datos?\n",
    "\n",
    "### Punto 1\n",
    "\n",
    "* Generalmente es util pensar en terminos de \"objetos\" de datos - cada objeto siendo algo que informa sobre el problema que estamos tratando (tweets, usuarios, hashtags, photos...)\n",
    "* Luego, fuera de las carecteristicas basicas, es muy importante recoger información temporal!\n",
    "* Tenemos problemas con la temporalidad, porque igual seria interesante saber información de los tweets cada dia desde que fue publicado, por ejemplo...\n",
    "\n",
    "### Punto 2\n",
    "\n",
    "* Entra dinamicas interesantes aqui - estamos tratando de datos \"externos\" que siempre es mas dificil que datos \"internos\" ya que controlamos los internos\n",
    "* Con datos externos es muy comun tener que explotar scraping de webs o de APIs - si es por API normalmente es más facil\n",
    "* Otro punto importante es exactamente como creamos nuestro proceso de scraping ya que influye sobre los datos que recogemos\n",
    "* El problema tipico con los APIs es que estamos limitado por el propio API (rate limiting, formatos, información expuesta...)\n",
    "\n",
    "## Como lo vamos a hacer?\n",
    "Ya he hecho esta tarea antes de la clase porque la descarga tarda lo suyo. He optado por el siguente patron:\n",
    "\n",
    "* Recoger unas cuentas con muchos seguidores (donde encontraremos muchos tweets populares)\n",
    "* Scrapear la información de estas cuentas a una fecha\n",
    "* X dias despues, recoger todos sus tweets entre la fecha de descarga de las cuentas y la fecha de hoy\n",
    "\n",
    "Lo hacemos asi para poder tener el número de seguidores etc. ANTES de crear los tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064c44fb-7072-4592-bba8-449f43751f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import twint\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "users_downloaded_at = '2021-08-11'\n",
    "download_tweets_from = '2021-08-12'\n",
    "download_tweets_to = '2021-08-19'\n",
    "\n",
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb0cb0c-6459-4a85-9335-68eab3af40a1",
   "metadata": {},
   "source": [
    "## Descargando los usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c1536f-d938-4345-8f5d-e972ac93e457",
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = [\n",
    "    'BarackObama', 'MichelleObama', 'Oprah', 'realmadrid', 'fcbarcelona', 'sanchezcastejon', 'theonion'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7135f48-6791-470a-9b1b-d271b8181ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "unfound_accounts = []\n",
    "not_worked = []\n",
    "for i, username in enumerate(handles):\n",
    "    if i % 100 == 0:\n",
    "        print(f'Done with {i}')\n",
    "    \n",
    "    c = twint.Config()\n",
    "    c.Username = username\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    \n",
    "    try:\n",
    "        twint.run.Lookup(c)\n",
    "    \n",
    "        results.append(twint.storage.panda.User_df)\n",
    "    except ValueError as e:\n",
    "        if 'Cannot find twitter account' in str(e):\n",
    "            unfound_accounts.append(username)\n",
    "        else:\n",
    "            raise\n",
    "    except twint.token.RefreshTokenException:\n",
    "        not_worked.append(username)\n",
    "        time.sleep(60)\n",
    "        \n",
    "        try:\n",
    "            twint.run.Lookup(c)\n",
    "            results.append(twint.storage.panda.User_df)\n",
    "        except:\n",
    "            print('Getting refresh errors!')\n",
    "\n",
    "results = pd.concat(results)\n",
    "results.to_csv(f'{datetime.today().strftime(\"%Y-%m-%d\")}-handles-data.csv', index=False)\n",
    "\n",
    "print(unfound_accounts)\n",
    "print(not_worked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64add6b3-ecd0-4c76-a810-14e9f038053f",
   "metadata": {},
   "source": [
    "## Descargando los Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bac7ca-a95f-46f7-b083-bec91e13dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_details = pd.read_csv(f'{users_downloaded_at}-handles-data.csv').drop_duplicates('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b3f27-5287-4bbf-b47a-83d8944a3bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "not_worked = []\n",
    "unfound_accounts = []\n",
    "for i, username in enumerate(user_details.username.dropna().unique()):\n",
    "    if i % 100 == 0:\n",
    "        print(f'Done with {i}')\n",
    "    \n",
    "    c = twint.Config()\n",
    "    c.Username = username\n",
    "    c.Since = download_tweets_from\n",
    "    c.Until = download_tweets_to\n",
    "    c.Pandas = True\n",
    "    c.Hide_output = True\n",
    "    \n",
    "    try:\n",
    "        twint.run.Profile(c)\n",
    "\n",
    "        results.append(twint.storage.panda.Tweets_df)\n",
    "    except ValueError as e:\n",
    "        if 'Cannot find twitter account' in str(e):\n",
    "            unfound_accounts.append(username)\n",
    "        else:\n",
    "            raise\n",
    "    except twint.token.RefreshTokenException:\n",
    "        not_worked.append(username)\n",
    "        time.sleep(60)\n",
    "\n",
    "        try:\n",
    "            twint.run.Profile(c)\n",
    "            results.append(twint.storage.panda.Tweets_df)\n",
    "        except:\n",
    "            print('Getting refresh errors!')\n",
    "        \n",
    "    \n",
    "results = pd.concat(results)\n",
    "results.to_csv(f'{users_downloaded_at}-{download_tweets_from}-{download_tweets_to}-tweets-data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4d7fc7-c75a-49d3-8985-2f0044d9b713",
   "metadata": {},
   "source": [
    "Cosas a mencionar (por si me olvido!):\n",
    "\n",
    "* Que cosas nos devuelve Twitter\n",
    "* La libreria de twint\n",
    "* Rate limiting y retries\n",
    "* Guardando lo que vas sacando\n",
    "* Scripts vs. notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768357ee-ff48-4f5d-b484-f648c921fb2d",
   "metadata": {},
   "source": [
    "## Creando nuestro data set\n",
    "Ahora tenemos unos datos sobre tweets - pero como los utilizamos para entender el problema? Analizarlos por supuesto!\n",
    "\n",
    "Cualquier persona que ha hecho hasta el analisis de datos mas basico en Excel sabe que es muy conveniente tener nuestros datos en tablas. Ya tenemos esto simplemente por como construimos el proceso inicial, pero tenemos los datos en 2 piezas! Esto dificulta ciertos analisis (pero es necesario para otros!) asi que tenemos que ver como resolver este punto.\n",
    "\n",
    "Primero, echamos un vistazo breve de los datos para entender que cosas hemos podido sacar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536fc974-00b8-409e-87c6-534738915fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_details = pd.read_csv(f'2021-08-11-handles-data.csv').drop_duplicates('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f58b739-9d94-425d-add8-6d23efe8863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_details.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f945c13-0f28-427c-95f1-b918390e1aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = pd.read_csv('2021-08-11-2021-08-12-2021-08-19-tweets-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80537db-13a1-48c1-aae9-a94a3fb370a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1126fa0e-05b6-4e95-96d6-848305dabe67",
   "metadata": {},
   "source": [
    "### Tarea: Como podemos sacar valor de estos datos?\n",
    "Hemos visto, más o menos que informacion somos capaces de sacar de Twitter - pero,\n",
    "\n",
    "1. Como nos ayuda en resolver nuestro problema?\n",
    "2. Siempre hablamos de \"big data\"? Tenemos aqui \"big data\"? Hace falta? Que perdemos?\n",
    "\n",
    "### Juntando los datos\n",
    "Ahora, queremos también juntar los datos para unos analisis que vamos a hacer en las semanas que vienen. Juntar datos es un concepto muy importante. La raiz de ML es permitir a una maquina aprender sobre un comportamiento - teniendo datos que reflejan este comportamiento. Parte de esto es tener nuestros diferentes \"fuentes\" de informacion sobre los comportamientos de interes - cada columna de cada objeto de datos aporta **algo** sobre nuestro comportamiento.\n",
    "\n",
    "Por esto, es muy importante poder \"juntar\" diferentes objetos y fuentes, para tener una foto más completo.\n",
    "\n",
    "#### Tarea: Como podemos juntar nuestros datos de tweets y los usuarios?\n",
    "\n",
    "* Follow up: Implica esto algo sobre la granularidad de los datos? Que pasa si nos interesa más entender los usuarios populares en vez de tweets populares?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c543e559-3f12-4f5c-8692-0ee927a20ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're doing it live!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
